services:
  letta:
    image: letta/letta:latest
    container_name: letta
    restart: unless-stopped
    ports:
      - "8283:8283"
    environment:
      - VLLM_API_BASE=http://vllm:8000
      - LETTA_LLM_MODEL=${LETTA_LLM_MODEL} # Replace with your model
      - LETTA_LLM_CONTEXT_WINDOW=8192
    depends_on:
      - vllm

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    restart: unless-stopped
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model ${LETTA_LLM_MODEL} --max_model_len=8192
    # Replace with your model
    ipc: host
