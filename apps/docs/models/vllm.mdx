---
title: 'vLLM'
---

<Tip>To use Letta with vLLM, set the enviornment variable `VLLM_API_BASE` to point to your vLLM ChatCompletitions server.</Tip>

## Setting up vLLM 
1. Download + install [vLLM](https://docs.vllm.ai/en/latest/getting_started/installation.html)
2. Launch a vLLM **OpenAI-compatible** API server using [the official vLLM documentation](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)

For example, if we want to use the model `dolphin-2.2.1-mistral-7b` from [HuggingFace](https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b), we would run:

```sh
python -m vllm.entrypoints.openai.api_server \
--model ehartford/dolphin-2.2.1-mistral-7b
```

vLLM will automatically download the model (if it's not already downloaded) and store it in your [HuggingFace cache directory](https://huggingface.co/docs/datasets/cache).


## Configuring with vLLM
<Accordion icon="square-terminal" title="CLI (pypi only)">
To configure your LLM model, modify the `[model]` section of your `~/.letta/config` file: 
```yaml ~/.letta/config
[model]
model_endpoint_type = vllm
model = ehartford/dolphin-2.2.1-mistral-7b
model_endpoint = http://localhost:8000
context_window = 8192
```
</Accordion>
<Accordion icon="docker" title="Docker Compose">
### Configuring with `docker compose`
The `compose.yaml` file uses enviornment variables to configure the LLM and embedding models. You can place these enviornment variables in either a `.env` file or set the variables. 

Configuring LLM endpoints:  
```
LETTA_LLM_ENDPOINT=http://host.docker.internal:8000
LETTA_LLM_ENDPOINT_TYPE=vllm
LETTA_LLM_MODEL=ehartford/dolphin-2.2.1-mistral-7b
LETTA_LLM_CONTEXT_WINDOW=8192
```
</Accordion>
<Accordion icon="python" title="Python SDK">
You can override the default LLM config (specified in `~/.letta/config`) by providing a new `LLMConfig` object to the `set_default_llm_config` method. 
```python
from memgpt import LLMConfig

client.set_default_llm_config(
    LLMConfig(
        model="ehartford/dolphin-2.2.1-mistral-7b",
        model_endpoint_type="vllm",
        model_endpoint="http://localhost:8000",
        context_window=8192
    )
)
```
</Accordion>

