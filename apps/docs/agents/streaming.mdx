---
title: Streaming support 
sidebarTitle: Message streaming
icon: "signal-stream"
iconType: "solid"
description: 'How to stream agent messages'
---

<Tip>Streaming is supported on the **REST API** and Python SDK **RESTClient**. Streaming support is not yet available on the Python SDK **LocalClient**.</Tip>

Messages from the **Letta server** can be **streamed** to the client. 
If you're building on the Letta API, enabling streaming allows your interface to update in real-time as the agent generates a response to an input message.

There are two kinds of streaming you can enable: **streaming agent steps** and **streaming tokens**.

## Streaming agent steps

When you send a message to the Letta server, the agent may run multiple steps while generating a response.
For example, an agent may run a search query, then use the results of that query to generate a response.

To enable streaming agent steps on the REST API, set the `stream_steps` parameter to `true` in your API request.
With `stream_steps` enabled, the response to the `POST` request will stream back as server-sent events (read more about SSE format [here](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events)):
```sh
# in this example, we're running a Letta server locally (on localhost:8283)
# the agent was already created and has ID: agent-687b9763-0eb0-4fcb-915d-42bd9e7560bf
curl --request POST \
  --url http://localhost:8283/v1/agents/agent-687b9763-0eb0-4fcb-915d-42bd9e7560bf/messages \
  --header 'Content-Type: application/json' \
  --data '{
  "messages": [
    {
      "role": "user",
      "text": "How is it going Mr. Bot?"
    }
  ],
  "stream_steps": true
}'
```
```
data: [DONE_GEN]

data: {"id":"...","date":"...","message_type":"internal_monologue","internal_monologue":"Charles just referred to me as Mr. Bot. Hmm, is it playful or does he perhaps misunderstand my nature?  Better clarify while being engaging."}

data: {"id":"...","date":"...","message_type":"function_call","function_call":{"name":"send_message","arguments":"{\n  \"message\": \"Ah, Charles! You've got it slightly mixed up. I'm Sam! As for how things are going, it's always a great day for a chat. But what about you \u2013 how are you doing today?\"\n}","function_call_id":"call_xHoMu2jCexf8mqlmfaY2xG5e"}}

data: {"id":"...","date":"...","message_type":"function_return","function_return":"None","status":"success","function_call_id":"call_xHoMu2jCexf8mqlmfaY2xG5e"}

data: [DONE_STEP]

data: [DONE]
```
In the response, we see the data returns in six chunks (note: `id` and `date` fields have been ommited for brevity):
<Steps>
  <Step title="[DONE_GEN]">
    Indicates the end of a single LLM inference step.
  </Step>
  <Step title="internal_monologue">
    A message with the agent's internal monologue: `"Charles just referred to me as Mr. Bot. Hmm, is it playful or does he perhaps misunderstand my nature?  Better clarify while being engaging."`.
  </Step>
  <Step title="function_call">
    A message with the function call that the agent is about to make (`send_message`) and the arguments that will be passed to the function. Note that the `message` arg contains the message to send to the user: `"Ah, Charles! You've got it slightly mixed up. I'm Sam! As for how things are going, it's always a great day for a chat. But what about you \u2013 how are you doing today?"`.
  </Step>
  <Step title="function_return">
    A message with the result of the function execution. In this case, the agent executed a simple function (`send_message`) which returned `None`. If the function call failed, the `status` field will be set to `error` and the `function_return` field will contain the error message.
  </Step>
  <Step title="[DONE_STEP]">
    Indicates the end of a single agent step. One step = one cycle of LLM inference = one action taken by the agent (e.g. editing memory or sending a message each are one step).
  </Step>
  <Step title="usage">
    A message with the usage statistics for the Letta agent run (number of tokens used, number of steps taken, etc).
  </Step>
  <Step title="[DONE]">
    Indicates the end of the POST SSE stream. If the agent decides to run multi-step reasoning, you will see multiple `[DONE_GEN]` and `[DONE_STEP]` messages before receiving the final `[DONE]` message.
  </Step>
</Steps>

## Streaming tokens 

You can also stream chunks of tokens from the agent as they are generated by the underlying LLM process by setting `stream_tokens` to `true` in your API request:
```sh
# using the same server and agent as in the previous example
# but now with streaming tokens enabled
curl --request POST \
  --url http://localhost:8283/v1/agents/agent-687b9763-0eb0-4fcb-915d-42bd9e7560bf/messages \
  --header 'Content-Type: application/json' \
  --data '{
  "messages": [
    {
      "role": "user",
      "text": "How is it going Mr. Bot?"
    }
  ],
  "stream_steps": true,
  "stream_tokens": true
}'
```

With token streaming enabled, the response will look very similar to the prior example (agent steps streaming), but instead of receiving complete messages, the client receives multiple messages with chunks of the response.
The client is responsible for reassembling the response from the chunks.
We've ommited most of the chunks for brevity:
```sh
data: {"id":"...","date":"...","message_type":"internal_monologue","internal_monologue":""}

data: {"id":"...","date":"...","message_type":"internal_monologue","internal_monologue":"Charles"}

data: {"id":"...","date":"...","message_type":"internal_monologue","internal_monologue":" seems"}

... (chunks ommited)

data: {"id":"...","date":"...","message_type":"internal_monologue","internal_monologue":" persona"}

data: {"id":"...","date":"...","message_type":"internal_monologue","internal_monologue":"."}

data: {"id":"...","date":"...","message_type":"function_call","function_call":{"name":"send_message","function_call_id":"c7d07b12-92ea-4379-b1f6-849c2"}}

data: {"id":"...","date":"...","message_type":"function_call","function_call":{"arguments":"{\n"}}

... (chunks ommited)

data: {"id":"...","date":"...","message_type":"function_call","function_call":{"arguments":"}"}}

data: [DONE_GEN]

data: {"id":"...","date":"...","message_type":"function_return","function_return":"None","status":"success","function_call_id":"c7d07b12-92ea-4379-b1f6-849c2"}

data: [DONE_STEP]

data: {"usage":{"completion_tokens":71,"prompt_tokens":2511,"total_tokens":2582,"step_count":1}}

data: [DONE]
```

## Tips on handling streaming in your client code

<Note>You can see the data types for streaming in the [agent docs](messages#message-types) and the Python reference for [LettaMessage](/python-reference/LettaMessage)</Note>

The data structure for token streaming is the same as for agent steps streaming (`LettaMessage`) - just instead of returning complete messages, the Letta server will return multiple messages each with a chunk of the response.
Because the format of the data looks the same, if you write your frontend code to handle tokens streaming, it will also work for agent steps streaming.

For example, if the Letta server is connected to multiple LLM backend providers and only a subset of them support LLM token streaming, you can use the same frontend code (interacting with the Letta API) to handle both streaming and non-streaming providers.
If you send a message to an agent with streaming enabled (both `stream_steps` and `stream_tokens` are `true`), the server will stream back `LettaMessage` objects with chunks if the selected LLM provider supports token streaming, and `LettaMessage` objects with complete strings if the selected LLM provider does not support token streaming.
