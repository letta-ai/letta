---
title: Streaming support 
sidebarTitle: Message streaming
icon: "signal-stream"
iconType: "solid"
description: 'How to stream agent messages'
---

<Tip>Streaming is supported on the **REST API** and Python SDK **RESTClient**. Streaming support is not yet available on the Python SDK **LocalClient**.</Tip>

Messages from the **Letta server** can be **streamed** to the client. 
If you're building on the Letta API, enabling streaming allows your interface to update in real-time as the agent generates a response to an input message.

There are two kinds of streaming you can enable: **streaming agent steps** and **streaming tokens**.

## Streaming agent steps

When you send a message to the Letta server, the agent may run multiple steps while generating a response.
For example, an agent may run a search query, then use the results of that query to generate a response.

To enable streaming agent steps on the REST API, set the `stream_steps` parameter to `true` in your API request: 
```curl
TODO
```

With `stream_steps` enabled, the response to the `POST` request will stream back as server-sent events (read more about SSE format [here](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events)):
```sh
TODO
```

## Streaming tokens 

You can also stream chunks of tokens from the agent as they are generated by the underlying LLM process by setting `stream_tokens` to `true` in your API request:
```curl
TODO
```

With token streaming enabled, the response will look very similar to the prior example (agent steps streaming), but instead of receiving complete messages, the client receives multiple messages with chunks of the response (the client is responsible for reassembling the response from the chunks):
```sh
TODO
```

## Tips on handling streaming in your client code

<Note>You can see the data types for streaming in the [agent docs](messages#message-types) and the Python reference for [LettaMessage](/python-reference/LettaMessage)</Note>

The data structure for token streaming is the same as for agent steps streaming (`LettaMessage`) - just instead of returning complete messages, the Letta server will return multiple messages each with a chunk of the response.
Because the format of the data looks the same, if you write your frontend code to handle tokens streaming, it will also work for agent steps streaming.

For example, if the Letta server is connected to multiple LLM backend providers and only a subset of them support LLM token streaming, you can use the same frontend code (interacting with the Letta API) to handle both streaming and non-streaming providers.
If you send a message to an agent with streaming enabled (both `stream_steps` and `stream_tokens` are `true`), the server will stream back `LettaMessage` objects with chunks if the selected LLM provider supports token streaming, and `LettaMessage` objects with complete strings if the selected LLM provider does not support token streaming.
