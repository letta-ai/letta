---
title: 'Model Configuration'
description: 'Configuring LLM backends'
icon: "microchip"
iconType: "solid"
---

Letta is designed to be model agnostic, so you can swap out backend models anytime. 

To configure LLM model endpoints, you need to specify the following environment variables: 
```
LETTA_LLM_ENDPOINT=http://host.docker.internal:11434
LETTA_LLM_ENDPOINT_TYPE=ollama
LETTA_LLM_MODEL=...
LETTA_LLM_CONTEXT_WINDOW=...
```

Similarly to configure embedding model endpoints, you need to specify the following: 
```
LETTA_EMBEDDING_ENDPOINT=http://host.docker.internal:11434
LETTA_EMBEDDING_ENDPOINT_TYPE=ollama
LETTA_EMBEDDING_MODEL=...
LETTA_EMBEDDING_DIM=...
```
 
## OpenAI 
To quickly get started with 
```
letta quickstart --backend openai
```

## Anthropic 
To quickly get started with Anthropic, you can run: 
```
letta quickstart --backend anthropic
```




## vLLM 


1. Download + install [vLLM](https://docs.vllm.ai/en/latest/getting_started/installation.html)
2. Launch a vLLM **OpenAI-compatible** API server using [the official vLLM documentation](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)

For example, if we want to use the model `dolphin-2.2.1-mistral-7b` from [HuggingFace](https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b), we would run:

```sh
python -m vllm.entrypoints.openai.api_server \
--model ehartford/dolphin-2.2.1-mistral-7b
```

vLLM will automatically download the model (if it's not already downloaded) and store it in your [HuggingFace cache directory](https://huggingface.co/docs/datasets/cache).
