---
title: 'Installation'
description: 'Install and setup Letta to run the server & ADE'
icon: "docker"
iconType: "solid"
---

## Run Letta with `pip`
<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/EOkpFDBNyEw"
  title="Install Letta with pip"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>

<Steps>
  <Step title="Install using PIP">
    To install Letta, run:
    ```
    pip install letta
    ```
  </Step>
  <Step title="Configure Letta">
    Run `letta configure` or `letta quickstart` to configure Letta.

    To customize the configuration (e.g. the LLM or embedding model to use), edit the `~/.letta/config` file.
    ```ini ~/.letta/config
    [model]
    model = gpt-4
    model_endpoint = https://api.openai.com/v1
    model_endpoint_type = openai
    context_window = 8192

    [embedding]
    embedding_endpoint_type = openai
    embedding_endpoint = https://api.openai.com/v1
    embedding_model = text-embedding-ada-002
    embedding_dim = 1536
    embedding_chunk_size = 300
    ```
  </Step>
  <Step title="Run the Letta server">
    To run the Letta server, run:
    ```sh
    letta run [--debug]
    ```
    You can now access the ADE (in your browser) and REST API server at `http://localhost:8283`.
  </Step>
</Steps>



## Run Letta with Docker

<Steps>
  <Step title="Download the docker container">
    To run the docker container, first clone the reposity for pull the contianer. 
    ```sh
    git clone https://github.com/cpacker/MemGPT
    ```
  </Step>
  <Step title="Set environment variables">
    Either set the environment variables in your shell on in a `.env` file. 
  <CodeGroup>
    ```bash .env file
    # To use OpenAI 
    OPENAI_API_KEY=...  

    # To use with Ollama 
    LETTA_LLM_ENDPOINT=http://host.docker.internal:11434
    LETTA_LLM_ENDPOINT_TYPE=ollama
    LETTA_LLM_MODEL=dolphin2.2-mistral:7b-q6_K
    LETTA_LLM_CONTEXT_WINDOW=8192
    LETTA_EMBEDDING_ENDPOINT=http://host.docker.internal:11434
    LETTA_EMBEDDING_ENDPOINT_TYPE=ollama
    LETTA_EMBEDDING_MODEL=mxbai-embed-large
    LETTA_EMBEDDING_DIM=512
    ```
    ```bash shell
    # To use OpenAI 
    export OPENAI_API_KEY=...  

    # To use with Ollama 
    export LETTA_LLM_ENDPOINT=http://host.docker.internal:11434
    export LETTA_LLM_ENDPOINT_TYPE=ollama
    export LETTA_LLM_MODEL=dolphin2.2-mistral:7b-q6_K
    export LETTA_LLM_CONTEXT_WINDOW=8192
    export LETTA_EMBEDDING_ENDPOINT=http://host.docker.internal:11434
    export LETTA_EMBEDDING_ENDPOINT_TYPE=ollama
    export LETTA_EMBEDDING_MODEL=mxbai-embed-large
    export LETTA_EMBEDDING_DIM=512
    ```
  </CodeGroup>
  </Step>
  <Step title="(Optional) View and modify the compose YAML file">
    You can view and modify the `compose.yaml` file, for example, if you would like to change the default ports or `pgvector` version.
  </Step>
  <Step title="Run Docker Compose">
    To start the Letta server, we use [Docker Compose](https://docs.docker.com/compose/), which runs both the Letta container and the database container:
    ```sh
    docker compose up 
    ```
    You can now access the ADE (in your browser) and REST API server at `http://localhost:8083`.
  </Step>
</Steps>
