---
title: Customize your agent memory
slug: guides/agents/memory
---

Letta agents have programmable in-context memory. This means a section of the context window is reserved for editable memory: context that can be edited by memory editing tools. Like standard system prompts, the memory also can be used to define the behavior of the agent and store personalization data. The key distinction is that this data can be modified over time.

# Memory
The in-context (i.e. core) memory of agents is represented by a `Memory` object. This memory object contains:
* A set of `Block` objects representing a segment of memory, with an associated character limit, label, and value
* A set of memory editing tools, which allow the agent to self-edit its own memory

## Agent Core Memory
The core memory of an agent is a set of blocks, with an associated character limit, label, and value.
The contents of core memory is always stored in the agent's context window, so the information is always provided to the LLM during inference.

Archival memory and recall memory are **not** stored in-context unless explicitly retrieved.

### Blocks
Blocks are the basic unit of core memory.
A set of blocks makes up the core memory.

Each block has:
* A `limit`, corresponding to the character limit of the block (i.e. how many characters in the context window can be used up by this block)
* A `value`, corresponding to the data represented in the context window for this block
* A `label`, corresponding to the type of data represented in the block (e.g. `human`, `persona`)


### Creating agents with memory
When you create an agent in Letta, you initialize the core memory of the agent using the "memory blocks" field.
Each block has a **value** (the memory contents), as well as a **label** which tells the agent what the memory is used about (e.g. "human" refers to memories about the human user).
<CodeGroup>
```python title="python" maxLines=50
# install letta_client with `pip install letta-client`
from letta_client import Letta

# create a client to connect to your local Letta Server
client = Letta(
  base_url="http://localhost:8283"
)

# create an agent with two basic self-editing memory blocks
agent_state = client.agents.create(
    memory_blocks=[
        {
          "label": "human",
          "value": "The human's name is Bob the Builder.",
          "limit": 5000
        },
        {
          "label": "persona",
          "value": "My name is Sam, the all-knowing sentient AI.",
          "limit": 5000
        }
    ],
    model="openai/gpt-4o-mini",
    embedding="openai/text-embedding-3-small"
)
```
```typescript maxLines=50 title="node.js"
// install letta-client with `npm install @letta-ai/letta-client`
import { LettaClient } from '@letta-ai/letta-client'

// create a client to connect to your local Letta Server
const client = new LettaClient({
  baseUrl: "http://localhost:8283"
});

// create an agent with two basic self-editing memory blocks
const agentState = await client.agents.create({
    memoryBlocks: [
        {
          label: "human",
          value: "The human's name is Bob the Builder.",
          limit: 5000
        },
        {
          label: "persona",
          value: "My name is Sam, the all-knowing sentient AI.",
          limit: 5000
        }
    ],
    model: "openai/gpt-4o-mini",
    embedding: "openai/text-embedding-3-small"
});
```
</CodeGroup>

## Shared Memory
You can create blocks independently of agents. This allows for multiple agents to be *attached* to a block. This allows of synchronized context windows accross agents, enabling shared memory.
<CodeGroup>
```python title="python" maxLines=50
# create a persisted block, which can be attached to agents
block = client.blocks.create(
    label="organization",
    value="Organization: Letta",
    limit=4000,
)

# create an agent with both a shared block and its own blocks
shared_block_agent1 = client.agents.create(
    name="shared_block_agent1",
    memory_blocks=[
        {
            "label": "persona",
            "value": "I am agent 1"
        },
    ],
    block_ids=[block.id],
    model="openai/gpt-4o-mini",
    embedding="openai/text-embedding-3-small"
)

# create another agent sharing the block
shared_block_agent2 = client.agents.create(
    name="shared_block_agent2",
    memory_blocks=[
        {
            "label": "persona",
            "value": "I am agent 2"
        },
    ],
    block_ids=[block.id],
    model="openai/gpt-4o-mini",
    embedding="openai/text-embedding-3-small"
)
```
```typescript maxLines=50 title="node.js"
// create a persisted block, which can be attached to agents
const block = await client.blocks.create({
    label: "organization",
    value: "Organization: Letta",
    limit: 4000,
});

// create an agent with both a shared block and its own blocks
const sharedBlockAgent1 = await client.agents.create({
    name: "shared_block_agent1",
    memoryBlocks: [
        {
            label: "persona",
            value: "I am agent 1"
        },
    ],
    blockIds: [block.id],
    model: "openai/gpt-4o-mini",
    embedding: "openai/text-embedding-3-small"

});

// create another agent sharing the block
const sharedBlockAgent2 = await client.agents.create({
    name: "shared_block_agent2",
    memoryBlocks: [
        {
            label: "persona",
            value: "I am agent 2"
        },
    ],
    blockIds: [block.id],
    model: "openai/gpt-4o-mini",
    embedding: "openai/text-embedding-3-small"
});
```
</CodeGroup>

## Stateful Workflows (advanced)
In some advanced usecases, you may want your agent to have persistent memory while not retaining conversation history.
For example, if you are using a Letta agent as a "workflow" that's run many times across many different users, you may not want to keep the conversation or event history inside of the message buffer.

You can create a stateful agent that does not retain conversation (event) history (i.e. a "stateful workflow") by setting the `message_buffer_autoclear` flag to `true` during [agent creation](/api-reference/agents/create). If set to `true` (default `false`), the message history will not be persisted in-context between requests (though the agent will still have access to core, archival, and recall memory).

```mermaid
flowchart LR
    Input["New Message (Event) Input"] --> Agent

    subgraph "Agent Memory"
        CoreMem["Core Memory"]
        RecallMem["Recall Memory"]
        ArchivalMem["Archival Memory"]
        MsgBuffer["Message Buffer"]
    end

    CoreMem --> Agent
    RecallMem --> Agent
    ArchivalMem --> Agent
    MsgBuffer --> Agent

    Agent --> Finish["Finish Step"]
    Finish -.->|"Clear buffer"| MsgBuffer

    style MsgBuffer fill:#f96,stroke:#333
    style Agent fill:#6f9,stroke:#333
    style Finish fill:#f66,stroke:#333
```
