---
title: Developer quickstart
subtitle: Create your first Letta agent and view it in the ADE
slug: quickstart
---

This quickstart will get guide you through creating your first Letta agent.
If you're interested in learning about Letta and how it works, [read more here](/letta-platform).

## Run the Letta Server
<Tip>
If you're using **Letta Cloud**, you can skip this step (you do not need to run your own Letta Server).
Instead, you'll simply need your **Letta Cloud API key**.
</Tip>

**Letta agents** live inside a **Letta Server**, which persists them to a database.
You can interact with the Letta agents inside your Letta Server with the [ADE](/agent-development-environment) (a visual interface), and connect your agents to external application via the [REST API](https://docs.letta.com/api-reference) and Python & TypeScript SDKs.

The recommended way to run the Letta Server is with Docker ([view official installation guide](https://docs.docker.com/get-docker)).
You can also install and run the Letta Server using `pip` ([guide here](/guides/server/pip)).

The Letta server can be connected to various LLM API backends (see our [Docker guide](/server/docker) for more details).
In this example, we'll use an OpenAI API key:
```sh
# replace `~/.letta/.persist/pgdata` with wherever you want to store your agent data
docker run \
  -v ~/.letta/.persist/pgdata:/var/lib/postgresql/data \
  -p 8283:8283 \
  -e OPENAI_API_KEY="your_openai_api_key" \
  letta/letta:latest
```

If you have many different LLM API keys, you can also set up a `.env` file instead and pass that to `docker run`:
```sh
# using a .env file instead of passing environment variables
docker run \
  -v ~/.letta/.persist/pgdata:/var/lib/postgresql/data \
  -p 8283:8283 \
  --env-file .env \
  letta/letta:latest
```

Once the Letta server is running, you can access it via port `8283` (e.g. sending REST API requests to `http://localhost:8283/v1`).
You can also connect your server to the Letta ADE to access and manage your agents in a visual interface.

## Access the [Letta ADE](/agent-development-environment) (Agent Development Environment)

<Note>
The Letta ADE is a graphical user interface for creating, deploying, interacting and observing with your Letta agents.
You can access the ADE at [https://app.letta.com](https://app.letta.com).
</Note>

The ADE can connect to self-hosted Letta servers (e.g. a Letta server running on your laptop), as well as the Letta Cloud service. When connected to a self-hosted / private server, the ADE uses the Letta REST API to communicate with your server.

If you're running a Letta server to power an end-user application (such as a customer support chatbot), you can use the ADE to test, debug, and observe the agents in your server. You can also use the ADE as a general chat interface to interacting with your Letta agents.
<img className="block w-300 dark:hidden" src="https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/example_ade_screenshot_light.png" />
<img className="hidden w-300 dark:block" src="https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/example_ade_screenshot.png" />

To connect the ADE with your local Letta server, simply navigate to [https://app.letta.com](https://app.letta.com) and you will see "Local server" as an option in the left panel (if your server is running):
<img className="block w-300 dark:hidden" src="https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/example_ade_screenshot_agents_light.png" />
<img className="hidden w-300 dark:block" src="https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/example_ade_screenshot_agents.png" />

For information on how to configure the ADE with a Letta server running on a remote server, refer to [our guide on remote servers](/guides/ade/setup).

## Creating an agent with the Letta API
Let's create an agent via the Letta API, which we can then view in the ADE (you can also use the ADE to create agents).

To create an agent we'll send a POST request to the Letta Server ([API docs](/api-reference/agents/create)).
In this example, we'll use `gpt-4o-mini` as the base LLM model, and `text-embedding-3-small` as the embedding model (this requires having configured both `OPENAI_API_KEY` on our Letta Server).

We'll also artificially set the context window limit to 16k, instead of the 128k default for `gpt-4o-mini` (this can improve stability and performance):
<CodeGroup>
```curl curl
curl -X POST http://localhost:8283/v1/agents/ \
     -H "Content-Type: application/json" \
     -d '{
  "memory_blocks": [
    {
      "value": "The human'\''s name is Bob the Builder.",
      "label": "human"
    },
    {
      "value": "My name is Sam, the all-knowing sentient AI.",
      "label": "persona"
    }
  ],
  "llm": "openai/gpt-4o-mini",
  "context_window_limit": 16000,
  "embedding": "openai/text-embedding-3-small"
}'
```
```python title="python" maxLines=50
# install letta_client with `pip install letta-client`
from letta_client import Letta

# create a client to connect to your local Letta Server
client = Letta(
  base_url="http://localhost:8283"
)

# create an agent with two basic self-editing memory blocks
agent_state = client.agents.create(
    memory_blocks=[
        {
          "label": "human",
          "value": "The human's name is Bob the Builder."
        },
        {
          "label": "persona",
          "value": "My name is Sam, the all-knowing sentient AI."
        }
    ],
    model="openai/gpt-4o-mini",
    context_window_limit=16000,
    embedding="openai/text-embedding-3-small"
)

# the AgentState object contains all the information about the agent
print(agent_state)
```
```typescript maxLines=50 title="node.js"
// install letta-client with `npm install @letta-ai/letta-client`
import { LettaClient } from '@letta-ai/letta-client'

// create a client to connect to your local Letta Server
const client = new LettaClient({
  baseUrl: "http://localhost:8283"
});

// create an agent with two basic self-editing memory blocks
const agentState = await client.agents.create({
    memoryBlocks: [
        {
          label: "human",
          value: "The human's name is Bob the Builder."
        },
        {
          label: "persona",
          value: "My name is Sam, the all-knowing sentient AI."
        }
    ],
    llm: "openai/gpt-4o-mini",
    contextWindowLimit: 16000,
    embedding: "openai/text-embedding-3-small"
});

// the AgentState object contains all the information about the agent
console.log(agentState);
```
</CodeGroup>

The response will include information about the agent, including its `id`:
```json
{
  "id": "agent-43f8e098-1021-4545-9395-446f788d7389",
  "name": "GracefulFirefly",
  ...
}
```

## Send a message to the agent with the Letta API
<Tip>
The Letta API supports streaming both agent *steps* and streaming *tokens*.
For more information on streaming, see [our guide on streaming](/guides/agents/streaming).
</Tip>
Let's try sending a message to the new agent! Replace `AGENT_ID` with the actual agent ID we received in the agent state ([route documentation](https://docs.letta.com/api-reference/agents/send-message)):
<CodeGroup>
```curl curl
curl --request POST \
  --url http://localhost:8283/v1/agents/$AGENT_ID/messages \
  --header 'Content-Type: application/json' \
  --data '{
  "messages": [
    {
      "role": "user",
      "text": "hows it going????"
    }
  ]
}'
```
```python title="python" maxLines=50
# send a message to the agent
response = client.agents.messages.send(
    agent_id=agent_state.id,
    messages=[
        {
            "role": "user",
            "text": "hows it going????"
        }
    ]
)

# the response object contains the messages and usage statistics
print(response)

# if we want to print the usage stats
print(response.usage)

# if we want to print the messages
for message in response.messages:
    print(message)
```
```typescript maxLines=50 title="node.js"
// send a message to the agent
const response = await client.agents.messages.send(
    agentState.id, {
        messages: [
            {
                role: "user",
                text: "hows it going????"
            }
        ]
    }
);

// the response object contains the messages and usage statistics
console.log(response);

// if we want to print the usage stats
console.log(response.usage)

// if we want to print the messages
for (const message of response.messages) {
    console.log(message);
}
```
</CodeGroup>

The response contains the agent's full response to the message, which includes reasoning steps (inner thoughts / chain-of-thought), tool calls, tool responses, and agent messages (directed at the user):
```json maxLines=50
{
  "messages": [
    {
      "id": "message-29d8d17e-7c50-4289-8d0e-2bab988aa01e",
      "date": "2024-12-12T17:05:56+00:00",
      "message_type": "reasoning_message",
      "reasoning": "Feeling energized to chat! Ready to connect and share some positive vibes with Bob the Builder."
    },
    {
      "id": "message-29d8d17e-7c50-4289-8d0e-2bab988aa01e",
      "date": "2024-12-12T17:05:56+00:00",
      "message_type": "assistant_message",
      "assistant_message": "Hey! I'm feeling great, thanks for asking! How about you? Whatâ€™s on your mind?"
    }
  ],
  "usage": {
    "completion_tokens": 56,
    "prompt_tokens": 2030,
    "total_tokens": 2086,
    "step_count": 1
  }
}
```

### Message Types
The `response` object contains the following attributes:
* `usage`: The usage of the agent after the message was sent (the prompt tokens, completition tokens, and total tokens)
* `message`: A list of `LettaMessage` objects, generated by the agent

#### `LettaMessage`
The `LettaMessage` object is a simplified version of the `Message` object stored in the database backend.
Since a `Message` can include multiple events like a chain-of-thought and function calls, `LettaMessage` simplifies messages to have the following types:
* `reasoning_message`: The inner monologue (chain-of-thought) of the agent
* `tool_call_message`: An agent's tool (function) call
* `tool_call_return`: The result of executing an agent's tool (function) call
* `assistant_message`: An agent calling the `send_message` tool to communicate with the user
* `system_message`: A system message (for example, an alert about the user logging in)
* `user_message`: A user message

The `assistant_message` message type is a convenience wrapper around the `tool_call_message` when the tool call is the predefined `send_message` tool that makes it easier to parse agent messages.
If you prefer to see the raw tool call even in the `send_message` case, you can set `use_assistant_message` to `false` in the request `config` (see the [endpoint documentation](/api-reference/agents/messages/send)).

## Viewing the agent in the ADE
We've created and messaged our first stateful agent. This agent exists in our Letta server, which means we can view it in the ADE (and continue the conversation there!).

If we open the ADE and click on "Agents", we should see our agent, as well as the message that we sent to it:
<img className="block w-300 dark:hidden" src="/images/ade_screenshot_chat_light.png" />
<img className="hidden w-300 dark:block" src="/images/ade_screenshot_chat.png" />

## Next steps

Congratulations! ðŸŽ‰ You just created and messaged your first stateful agent with Letta, using both the Letta ADE, API, and Python/Typescript SDKs.

Now that you've succesfully created a basic agent with Letta, you're ready to start building more complex agents and AI applications.

<CardGroup cols={1}>
  <Card title="Stateful Agents" icon="fa-sharp fa-solid fa-alien-8bit" iconPosition='left' href="/guides/agents/overview">
Learn more about building Stateful Agents in Letta
  </Card>
  <Card title="ADE Guide" icon="fa-sharp fa-light fa-browser" iconPosition='left' href="/guides/ade/setup">
Learn how to configure agents, tools, and memory in the ADE
  </Card>
  <Card title="Full API and SDK Reference" icon="fa-sharp fa-light fa-code" iconPosition='left' href="/api-reference/overview">
View the Letta API and Python/TypeScript SDK reference
  </Card>
  <Card title="Agent Templates" icon="fa-sharp fa-solid fa-rocket" iconPosition='left' href="/guides/templates/overview">
Create common starting points for agents in production settings
  </Card>
</CardGroup>
